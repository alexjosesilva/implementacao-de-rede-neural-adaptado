{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexjosesilva/implementacao-de-rede-neural-adaptado/blob/main/Projeto_final_Implementa%C3%A7%C3%A3o_de_Rede_Neural_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GA6CPSkVlg76"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Projeto Final - Implementação de Rede Neural**\n",
        "\n",
        "Equipe:\n",
        "1.   Alex José Silva\n",
        "\n",
        "\n",
        "Gera:\n",
        " - métricas no console\n",
        " - comparação com baseline do exemplo original\n",
        " - gráfico da fronteira de decisão (para datasets 2D)\n",
        " - arquivo results.json com os números\n",
        "\n"
      ],
      "metadata": {
        "id": "-oSkdkVTlmiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import SGD, Adam\n",
        "\n",
        "# ------------------------------\n",
        "# Utilitários\n",
        "# ------------------------------\n",
        "\n",
        "ACTIVATIONS = {\"relu\": \"relu\", \"sigmoid\": \"sigmoid\", \"tanh\": \"tanh\", \"elu\": \"elu\", \"selu\": \"selu\", \"linear\": \"linear\"}\n",
        "\n",
        "\n",
        "def build_model(input_dim: int, layers: list[int], activations: list[str], output_activation: str = \"sigmoid\", lr: float = 0.001, optimizer: str = \"adam\"):\n",
        "    model = Sequential()\n",
        "    if not layers:\n",
        "        raise ValueError(\"Forneça ao menos uma camada oculta em --layers\")\n",
        "    if len(activations) == 1 and len(layers) > 1:\n",
        "        activations = activations * len(layers)\n",
        "    if len(activations) != len(layers):\n",
        "        raise ValueError(\"Número de funções de ativação deve bater com número de camadas ocultas\")\n",
        "\n",
        "    # Primeira camada com input_dim\n",
        "    model.add(Dense(layers[0], input_dim=input_dim, activation=ACTIVATIONS.get(activations[0], activations[0])))\n",
        "    # Demais camadas\n",
        "    for units, act in zip(layers[1:], activations[1:]):\n",
        "        model.add(Dense(units, activation=ACTIVATIONS.get(act, act)))\n",
        "    # Saída binária\n",
        "    model.add(Dense(1, activation=output_activation))\n",
        "\n",
        "    if optimizer.lower() == \"sgd\":\n",
        "        opt = SGD(learning_rate=lr)\n",
        "    else:\n",
        "        opt = Adam(learning_rate=lr)\n",
        "    model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])  # melhor que MSE p/ classificação\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_dataset(name: str, n_samples: int, noise: float, random_state: int = 42):\n",
        "    name = name.lower()\n",
        "    if name == \"moons\":\n",
        "        X, y = datasets.make_moons(n_samples=n_samples, noise=noise, random_state=random_state)\n",
        "        two_d = True\n",
        "    elif name == \"circles\":\n",
        "        X, y = datasets.make_circles(n_samples=n_samples, noise=noise, factor=0.5, random_state=random_state)\n",
        "        two_d = True\n",
        "    elif name == \"iris\":\n",
        "        iris = datasets.load_iris()\n",
        "        # binariza: classe 0 vs não-0 só para manter saída sigmoide\n",
        "        y = (iris.target != 0).astype(int)\n",
        "        # usa duas features p/ plot 2D\n",
        "        X = iris.data[:, [2, 3]]  # petal length, petal width\n",
        "        two_d = True\n",
        "    else:\n",
        "        raise ValueError(\"Dataset inválido. Use: moons | circles | iris\")\n",
        "    return X.astype(np.float32), y.astype(int), two_d\n",
        "\n",
        "\n",
        "def plot_decision_boundary(model, X, y, title: str, outpath: Path):\n",
        "    # Apenas para dados 2D\n",
        "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
        "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300), np.linspace(y_min, y_max, 300))\n",
        "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "    Z = model.predict(grid, verbose=0)\n",
        "    Z = (Z.ravel() > 0.5).astype(int)\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor=\"k\")\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(outpath)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def run_baseline(X, y, lr: float, epochs: int, batch_size: int, random_state: int = 42):\n",
        "    # Replica (aprox.) a cabeça do exemplo4.py: Dense(5,relu)->Dense(5,tanh)->Dense(1,sigmoid), SGD lr=0.1, MSE\n",
        "    scaler = StandardScaler()\n",
        "    Xs = scaler.fit_transform(X)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(Xs, y, test_size=0.3, stratify=y, random_state=random_state)\n",
        "\n",
        "    baseline = Sequential()\n",
        "    baseline.add(Dense(5, input_dim=X.shape[1], activation='relu'))\n",
        "    baseline.add(Dense(5, activation='tanh'))\n",
        "    baseline.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    opt = SGD(learning_rate=lr)\n",
        "    baseline.compile(loss='mean_squared_error', optimizer=opt, metrics=['accuracy'])\n",
        "    baseline.fit(X_train, y_train, epochs=epochs, verbose=False, batch_size=batch_size)\n",
        "    y_pred = (baseline.predict(X_test, verbose=0).ravel() > 0.5).astype(int)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    return acc, cm\n",
        "\n",
        "\n",
        "def main(dataset='moons', n_samples=300, noise=0.2, layers=[8, 8], activations=['relu', 'relu'], epochs=200, batch_size=16, lr=0.01, optimizer='adam', cv=0, seed=42):\n",
        "    outdir = Path('outputs')\n",
        "    outdir.mkdir(exist_ok=True)\n",
        "\n",
        "    X, y, two_d = load_dataset(dataset, n_samples, noise, seed)\n",
        "\n",
        "    # Escalonamento\n",
        "    scaler = StandardScaler()\n",
        "    Xs = scaler.fit_transform(X)\n",
        "\n",
        "    results = {\"dataset\": dataset, \"n_samples\": int(len(X)), \"noise\": float(noise)}\n",
        "\n",
        "    # ------------------------------\n",
        "    # Baseline (do exemplo original)\n",
        "    # ------------------------------\n",
        "    base_acc, base_cm = run_baseline(X, y, lr=0.1, epochs=100, batch_size=5)  # fiel ao exemplo\n",
        "    results[\"baseline_acc\"] = float(base_acc)\n",
        "    results[\"baseline_cm\"] = base_cm.tolist()\n",
        "\n",
        "    # ------------------------------\n",
        "    # Modelo customizado\n",
        "    # ------------------------------\n",
        "    if cv and cv > 1:\n",
        "        skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=seed)\n",
        "        accs = []\n",
        "        for fold, (tr, te) in enumerate(skf.split(Xs, y), start=1):\n",
        "            model = build_model(Xs.shape[1], layers, activations, lr=lr, optimizer=optimizer)\n",
        "            model.fit(Xs[tr], y[tr], epochs=epochs, verbose=False, batch_size=batch_size)\n",
        "            yhat = (model.predict(Xs[te], verbose=0).ravel() > 0.5).astype(int)\n",
        "            accs.append(accuracy_score(y[te], yhat))\n",
        "        acc = float(np.mean(accs))\n",
        "        results[\"custom_acc_cv_mean\"] = acc\n",
        "        print(f\"Acurácia média (CV {cv}): {acc:.4f}\")\n",
        "    else:\n",
        "        X_train, X_test, y_train, y_test = train_test_split(Xs, y, test_size=0.3, stratify=y, random_state=seed)\n",
        "        model = build_model(Xs.shape[1], layers, activations, lr=lr, optimizer=optimizer)\n",
        "        model.fit(X_train, y_train, epochs=epochs, verbose=False, batch_size=batch_size)\n",
        "        yhat = (model.predict(X_test, verbose=0).ravel() > 0.5).astype(int)\n",
        "        acc = accuracy_score(y_test, yhat)\n",
        "        cm = confusion_matrix(y_test, yhat)\n",
        "        print(classification_report(y_test, yhat, digits=4))\n",
        "        results[\"custom_acc\"] = float(acc)\n",
        "        results[\"custom_cm\"] = cm.tolist()\n",
        "        print(f\"Acurácia (custom): {acc:.4f}\")\n",
        "\n",
        "        # Fronteira de decisão (quando possível)\n",
        "        if two_d:\n",
        "            grid_title = f\"{dataset} | camadas {layers} | acts {activations}\"\n",
        "            plot_decision_boundary(model, scaler.transform(X), y, grid_title, outdir / f\"decision_{dataset}.png\")\n",
        "\n",
        "    # Comparação\n",
        "    if \"custom_acc\" in results:\n",
        "        delta = results[\"custom_acc\"] - results[\"baseline_acc\"]\n",
        "        print(f\"\\nComparação vs baseline: Δacc = {delta:+.4f}\")\n",
        "        results[\"delta_acc_vs_baseline\"] = float(delta)\n",
        "    elif \"custom_acc_cv_mean\" in results:\n",
        "        delta = results[\"custom_acc_cv_mean\"] - results[\"baseline_acc\"]\n",
        "        print(f\"\\nComparação vs baseline (CV): Δacc = {delta:+.4f}\")\n",
        "        results[\"delta_acc_vs_baseline\"] = float(delta)\n",
        "\n",
        "    # Persistir resultados\n",
        "    with open(outdir / 'results.json', 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "    print(\"Resultados salvos em outputs/results.json\")\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLoQZ1-ktoyi",
        "outputId": "ec6c9d87-298a-4c27-852c-aa495fc3c87f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     1.0000    0.9111    0.9535        45\n",
            "           1     0.9184    1.0000    0.9574        45\n",
            "\n",
            "    accuracy                         0.9556        90\n",
            "   macro avg     0.9592    0.9556    0.9555        90\n",
            "weighted avg     0.9592    0.9556    0.9555        90\n",
            "\n",
            "Acurácia (custom): 0.9556\n",
            "\n",
            "Comparação vs baseline: Δacc = +0.0000\n",
            "Resultados salvos em outputs/results.json\n"
          ]
        }
      ]
    }
  ]
}